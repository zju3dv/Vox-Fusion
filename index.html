<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="Dense Tracking and Mapping with Voxel-based Neural Implicit Representation.">
    <meta name="keywords" content="SLAM, NeRF, Dense">
    <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
    <title>Vox-Fusion: Dense Tracking and Mapping with Voxel-based Neural Implicit Representation</title>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./assets/css/main.css" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.css" rel="stylesheet"
        type='text/css'>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="icon" href="./assets/imgs/favicon.ico">
</head>

<body>
    <div class="content m-3">
        <div class="title text-center display-5 mb-5">Vox-Fusion: Dense Tracking and Mapping with Voxel-based Neural
            Implicit Representation
        </div>

        <div class="authors d-flex flex-wrap justify-content-center">
            <div class="text-center px-2 h4"><a href="https://yangxingrui.com">*Xingrui Yang</a><sup>1</sup></div>
            <div class="text-center px-2 h4"><a href="https://garylidd.github.io/">*Hai Li</a><sup>1</sup> </div>
            <div class="text-center px-2 h4"><a href="https://zhaihongjia.github.io/">Hongjia Zhai</a><sup>1</sup>
            </div>
            <div class="text-center px-2 h4"> <a href="https://yuhangming.github.io/">Yuhang Ming</a><sup>2</sup></div>
            <div class="text-center px-2 h4"><a>Yuqian Liu</a><sup>3</sup> </div>
            <div class="text-center px-2 h4"><a
                    href="http://www.cad.zju.edu.cn/home/gfzhang/">GuofengZhang</a><sup>1</sup></div>
        </div>

        <div class="text-center px-2 h4 mb-3"><em>*equal contribution</em></div>
        <div class="affiliations d-flex flex-wrap justify-content-center pt-2">
            <div class="text-center mx-2 h4"><sup>1</sup>State Key Lab of CAD&CG, Zhejiang University, China</div>
            <div class="text-center mx-2 h4"><sup>2</sup>University of Bristol, UK</div>
            <div class="text-center mx-2 h4"><sup>3</sup>Autonomous Driving Group, SenseTime</div>
        </div>

        <div class="links d-flex flex-wrap justify-content-center pt-1">
            <a class="btn btn-primary btn-lg mx-2" href="https://arxiv.org/abs/2210.15858"><i
                    class="ai ai-arxiv px-2"></i>ArXiv</a>
            <a class="btn btn-primary btn-lg mx-2" href="https://github.com/zju3dv/Vox-Fusion"><i
                    class="fa fa-github px-2"></i>Code</a>
        </div>

        <div class="teaser text-center mt-5">
            <video class="border border-3" autoplay muted loop playsinline width="100%">
                <source src="./assets/video/teaser.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <p>Our Vox-Fusion uses a explicit sparse voxel grid representation to reconstruct dense geometry and color
                from
                RGB-D images.</p>
        </div>

        <div class="abstract pt-5 justify-content-center">
            <div class="text-center h2">Abstract</div>
            <p>
                In this work, we present a dense tracking and mapping system named Vox-Fusion, which seamlessly fuses
                neural
                implicit representations with traditional volumetric fusion methods. Our approach is inspired by the
                recently
                developed implicit mapping and positioning system and further extends the idea so that it can be freely
                applied
                to practical scenarios. Specifically, we leverage a voxel-based neural implicit surface representation
                to
                encode
                and optimize the scene inside each voxel. Furthermore, we adopt an octree-based structure to divide the
                scene
                and support dynamic expansion, enabling our system to track and map arbitrary scenes without knowing the
                environment like in previous works. Moreover, we proposed a high-performance multi-process framework to
                speed up
                the method, thus supporting some applications that require realtime performance. The evaluation results
                show
                that our methods can achieve better accuracy and completeness than previous methods. We also show that
                our
                Vox-Fusion can be used in augmented reality and virtual reality applications.
            </p>
        </div>

        <div class="video mt-5">
            <div class="text-center h2">Video</div>
            <div class="publication-video">
                <iframe src="https://www.youtube.com/embed/Prp28y1b2Qs" width="100%" title="YouTube video player"
                    frameborder="0"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                    allowfullscreen></iframe>
            </div>
        </div>

        <div class="abstract pt-5 justify-content-center">
            <div class="text-center h2">Correction</div>
            <p class="text-bold">
                <b>
                    We found a bug in the evaluation script which affected the estimated pose accuracy in Tables 1 and 3
                    in the original paper. We have corrected this problem and re-run the results with updated
                    configurations. The corrected results are comparable (even better for Replica dataset) to the
                    originally reported results in the paper, which do not affect the contribution and conclusion of our
                    work. We have updated the <a href="https://arxiv.org/abs/2210.15858">arxiv version</a> of our paper
                    and publish
                    all the latest results (including mesh, pose, gt, eval scripts and training configs) on <a
                        href="https://zjueducn-my.sharepoint.com/:f:/g/personal/garyli_zju_edu_cn/EgEhBqp29R1Gl6kREj88nQ4BAzS_ezOFtiub6ZsvywO4og?e=QoRpr5">OneDrive</a>
                    in case anyone wants to reproduce our results and compare them using different metrics. You can also
                    find the updated Tables 1 & 3 below.
                </b>
            </p>
            <img class="img-fluid" src="assets/imgs/pose_replica.png" />
            <img class="img-fluid mx-auto d-block" src="assets/imgs/pose_scannet.png" />
        </div>

        <div class="bibtex mt-5">
            <h2>BibTeX</h2>
            <pre><code>
@article{yang2022voxfusion,
    title={Vox-Fusion: Dense Tracking and Mapping with Voxel-based Neural Implicit Representation},
    author={Yang, Xingrui and Li, Hai and Zhai, Hongjia and Ming, Yuhang and Liu, Yuqian and Zhang, Guofeng},
    journal={arXiv preprint arXiv:2210.15858},
    year={2022}
}</code></pre>
        </div>


        <div class="ackownledgement mt-5">
            <div class="h2">Ackownledgement</div>
            <p>We thank the authors of iMap for providing the Replica evaluation datasets. We would also thank the
                authors of NICE-SLAM for open sourcing their work. </p>
            <p>This webpage design is inspired by <a href="https://github.com/nerfies/nerfies.github.io">Nerfie</a></p>
        </div>
    </div>
</body>